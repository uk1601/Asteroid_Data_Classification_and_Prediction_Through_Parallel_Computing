{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cfdbbb6-c821-4919-b6eb-0a8591f85910",
   "metadata": {},
   "source": [
    "# Asteroid Class Prediction Leveraging Parallelization Techniques \n",
    "\n",
    "<div style=\"text-align: right\">  \n",
    "<strong>Uday Kiran Dasari</strong> \n",
    "</div>\n",
    "<br/>\n",
    "<center>\n",
    "<img src=\"Asteroid.jpg\" width=700 />\n",
    "</center>\n",
    "<br/>\n",
    "\n",
    "**Link to Dataset used**: [**Asteroid Dataset**](https://www.kaggle.com/datasets/sakhawat18/asteroid-dataset/data)\n",
    "\n",
    "### Dataset Description\n",
    "\n",
    "- **SPK-ID:** Object primary SPK-ID\n",
    "- **Object ID:** Object internal database ID\n",
    "- **Object fullname:** Object full name/designation\n",
    "- **pdes:** Object primary designation\n",
    "- **name:** Object IAU name\n",
    "- **NEO:** Near-Earth Object (NEO) flag\n",
    "- **PHA:** Potentially Hazardous Asteroid (PHA) flag\n",
    "- **H:** Absolute magnitude parameter\n",
    "- **Diameter:** Object diameter (from equivalent sphere) km Unit\n",
    "- **Albedo:** Geometric albedo\n",
    "- **Diameter_sigma:** 1-sigma uncertainty in object diameter km Unit\n",
    "- **Orbit_id:** Orbit solution ID\n",
    "- **Epoch:** Epoch of osculation in modified Julian day form\n",
    "- **Equinox:** Equinox of reference frame\n",
    "- **e:** Eccentricity\n",
    "- **a:** Semi-major axis au Unit\n",
    "- **q:** Perihelion distance au Unit\n",
    "- **i:** Inclination; angle with respect to x-y ecliptic plane\n",
    "- **tp:** Time of perihelion passage TDB Unit\n",
    "- **moid_ld:** Earth Minimum Orbit Intersection Distance au Unit\n",
    "-y ecliptic plane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b659a10d-eccf-4246-b99d-ea7233fef01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic imports\n",
    "import os\n",
    "import time\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm    \n",
    "    \n",
    "# Visualization imports\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import multiprocessing\n",
    "    \n",
    "# Scikit-learn imports\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "    \n",
    "# Dask-related imports\n",
    "import dask\n",
    "import dask.array as da\n",
    "import dask.dataframe as dd\n",
    "from dask_ml.impute import SimpleImputer\n",
    "from dask.distributed import config\n",
    "from dask_ml.model_selection import train_test_split\n",
    "    \n",
    "# Set the daemon configuration for Dask workers\n",
    "dask.config.set({'distributed.worker.daemon': False})\n",
    "    \n",
    "# PyTorch imports\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.distributed as dist\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "535a0953-83b8-469f-bbbf-7ac2f627007f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloading(path='./dataset.csv'):\n",
    "    astro_ds = dd.read_csv(path)\n",
    "    # Dropping irrelavant columns early in the process\n",
    "    columns_to_drop = ['id', 'full_name', 'pdes', 'name', 'prefix', 'neo', 'pha', 'orbit_id', 'equinox']\n",
    "    astro_ds = astro_ds.drop(columns_to_drop, axis=1)\n",
    "    print(\"Data Loaded and dropped irrelavant columns!!\")\n",
    "    return astro_ds\n",
    "\n",
    "def nullvalue_handling(astro_ds):\n",
    "    # Prepare features for imputation (excluding the target 'class')\n",
    "    X = astro_ds.drop(['class'], axis=1)\n",
    "    y = astro_ds['class']\n",
    "    # Applying SimpleImputer from Dask-ML to handle missing values in parallel\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "    # Since Dask works with lazy evaluation, use compute() to perform the computation\n",
    "    X_imputed = imputer.fit_transform(X)\n",
    "    # Ensure that imputation and other transformations are computed efficiently\n",
    "\n",
    "    with tqdm(total=2, desc=\"Computing DataFrames\") as pbar:\n",
    "        X_imputed = X_imputed.compute()\n",
    "        pbar.update(1)  # Update progress after computing X_imputed\n",
    "        y = y.compute()\n",
    "        pbar.update(1)  # Update progress after computing y\n",
    "    print(\"Null Value Handling Done!\")\n",
    "    return X_imputed,y\n",
    "\n",
    "def feature_importance(X_imputed,y):\n",
    "    # Split data into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_imputed, y, test_size=0.2,shuffle =True ,random_state=42)\n",
    "    # Initialize the ExtraTreesClassifier\n",
    "    # We can adjust n_estimators, max_depth, and other parameters as needed\n",
    "    etc = ExtraTreesClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "    \n",
    "    # To leverage parallel computation with multiprocessing when performing model fitting\n",
    "    with joblib.parallel_backend('multiprocessing'):\n",
    "        #Fit the model\n",
    "        etc.fit(X_train, y_train)\n",
    "    # Compute and print feature importances\n",
    "    feature_importances = etc.feature_importances_\n",
    "    \n",
    "    threshold = np.mean(feature_importances)  # Define your threshold here\n",
    "    \n",
    "    # Selecting features with importance greater than the threshold\n",
    "    selected_features = [feature for feature, importance in zip(X_train.columns, feature_importances) if importance > threshold]\n",
    "    \n",
    "    # Include 'class' in selected features\n",
    "    selected_features.append('class')\n",
    "    \n",
    "    # Filter the original Dask DataFrame to include only selected features\n",
    "    astro_ds_filtered = astro_ds[selected_features]\n",
    "    print(\"Feature Extraction done!\")\n",
    "    return astro_ds_filtered\n",
    "\n",
    "\n",
    "def dataprep(X_train,X_test,y_train,y_test):\n",
    "    # Scaling the data using Dask-ML's StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    # Encoding the target label using LabelEncoder\n",
    "    encoder = LabelEncoder()\n",
    "    y_train_encoded = encoder.fit_transform(y_train)\n",
    "    y_test_encoded = encoder.transform(y_test)\n",
    "    print(\"Scaling and Encoding Done!\")\n",
    "    return X_train_scaled,X_test_scaled,y_train_encoded,y_test_encoded\n",
    "\n",
    "def tensordataset(X_train, y_train, X_test, y_test):\n",
    "    # Convert test and train sets to tensors\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "    \n",
    "    #Generation of Train and Test Tensor dataset    \n",
    "    train_dataset=TensorDataset(X_train_tensor,y_train_tensor)\n",
    "    val_dataset=TensorDataset(X_test_tensor,y_test_tensor)\n",
    "    input_size=X_train_tensor.shape[1]\n",
    "    \n",
    "    print(f\"Tensor Conversion and dataset prep done!!Input Size:{input_size}\")\n",
    "    return train_dataset,val_dataset,input_size\n",
    "\n",
    "def plot_metrics(train_losses, val_losses, train_accuracies, val_accuracies):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_accuracies, label='Training Accuracy')\n",
    "    plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "265e46fb-0293-4f54-9e9b-e02ff004d0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)  # Linear layer mapping input to hidden layer\n",
    "        self.relu = nn.ReLU()  # ReLU activation function\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)  # Linear layer mapping hidden layer to output classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)  # Pass input through first linear layer\n",
    "        out = self.relu(out)  # Apply ReLU activation function\n",
    "        out = self.fc2(out)  # Pass through second linear layer to get class scores\n",
    "        return out\n",
    "\n",
    "class RobustNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(RobustNN, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size)  # First linear layer\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size)  # Batch normalization for first hidden layer\n",
    "        self.relu = nn.ReLU()  # ReLU activation function\n",
    "        self.dropout = nn.Dropout(0.5)  # Dropout for regularization with 50% probability\n",
    "        self.layer2 = nn.Linear(hidden_size, hidden_size // 2)  # Second linear layer reducing size\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_size // 2)  # Batch normalization for second hidden layer\n",
    "        self.layer3 = nn.Linear(hidden_size // 2, num_classes)  # Final linear layer to output classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)  # Pass input through the first layer\n",
    "        x = self.bn1(x)  # Apply batch normalization\n",
    "        x = self.relu(x)  # Apply ReLU activation\n",
    "        x = self.dropout(x)  # Apply dropout\n",
    "        x = self.layer2(x)  # Pass through the second linear layer\n",
    "        x = self.bn2(x)  # Apply batch normalization\n",
    "        x = self.relu(x)  # Apply ReLU activation\n",
    "        x = self.layer3(x)  # Output layer to get class scores\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0abdb80-b916-4f60-972c-2b00d5371f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup(rank, world_size):\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '65150'\n",
    "    torch.distributed.init_process_group('nccl', rank=rank, world_size=world_size)\n",
    "    torch.cuda.set_device(rank)  # set the current CUDA device using the rank\n",
    "    print(f\"[Rank {rank}] Initialization complete. Using world size {world_size}.\")\n",
    "def cleanup():\n",
    "    dist.destroy_process_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9df8e33a-8f1d-4fde-b86b-90ae0232460a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(rank, world_size, input_size, train_dataset, val_dataset, batch_size=32, num_epochs=1):\n",
    "    setup(rank, world_size)  # Initialize DDP environment\n",
    "\n",
    "    # Initialize model and move it to the specified device (GPU)\n",
    "    model = RobustNN(input_size=input_size, hidden_size=13, num_classes=13).to(rank)\n",
    "    # Wrapped in DDP to synchronize gradients\n",
    "    ddp_model = DDP(model, device_ids=[rank])\n",
    "    \n",
    "    # Create samplers and loaders for training and validation datasets\n",
    "    train_sampler = DistributedSampler(train_dataset, num_replicas=world_size, rank=rank, shuffle=False)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, sampler=train_sampler, pin_memory=False, num_workers=0)\n",
    "    val_sampler = DistributedSampler(val_dataset, num_replicas=world_size, rank=rank, shuffle=False)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, sampler=val_sampler, pin_memory=False, num_workers=0)\n",
    "    \n",
    "    # Setup optimizer and loss function\n",
    "    optimizer = optim.Adam(ddp_model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Metrics to keep track of progress\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "    epoch_times = []\n",
    "    \n",
    "    total_start_time = time.time()\n",
    "    # Training loop\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        epoch_start_time = time.time()\n",
    "        train_sampler.set_epoch(epoch)  # Ensures proper shuffling per epoch\n",
    "        train_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        ddp_model.train()  # Set model to training mode\n",
    "\n",
    "        for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(rank), labels.to(rank)  # Move data to the correct device\n",
    "            optimizer.zero_grad()  # Clear gradients\n",
    "            outputs = ddp_model(inputs)  # Forward pass\n",
    "            loss = criterion(outputs, labels)  # Compute loss\n",
    "            loss.backward()  # Backpropagate error\n",
    "            optimizer.step()  # Update weights\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Log training metrics\n",
    "        epoch_loss = train_loss / len(train_loader)\n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accuracies.append(100 * correct / total)\n",
    "        print(f\"[Rank {rank}] Epoch {epoch+1} average loss: {epoch_loss}\")\n",
    "\n",
    "        # Validation step\n",
    "        ddp_model.eval()  # Set model to evaluation mode\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(rank), labels.to(rank)\n",
    "                outputs = ddp_model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Log validation metrics\n",
    "        val_losses.append(val_loss / len(val_loader))\n",
    "        val_accuracies.append(100 * correct / total)\n",
    "        print(f\"[Rank {rank}] Epoch {epoch+1} validation loss: {val_loss / len(val_loader)}\")\n",
    "        print(f\"[Rank {rank}] Epoch {epoch+1} validation accuracy: {100 * correct / total:.4f}\")\n",
    "\n",
    "        ddp_model.train()  # Set model back to training mode\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        epoch_times.append(epoch_time)\n",
    "        print(f\"Rank {rank} and Epoch {epoch+1} Time Taken:{epoch_time:.2f} seconds\")\n",
    "\n",
    "    total_training_time = time.time() - total_start_time\n",
    "    print(f\"Rank {rank}: Total training time: {total_training_time:.2f} seconds\")\n",
    "    \n",
    "    cleanup()  # Clean up DDP setup\n",
    "    #if rank == 0:  # Plot metrics if this is the main process\n",
    "    #    plot_metrics(train_losses, val_losses, train_accuracies, val_accuracies)\n",
    "\n",
    "    print(\"Training Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa046d9b-7f71-4e81-9be0-c31b9b1e3143",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loaded and dropped irrelavant columns!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing DataFrames: 100%|██████████| 2/2 [00:03<00:00,  1.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null Value Handling Done!\n",
      "Feature Extraction done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing DataFrames: 100%|██████████| 2/2 [00:03<00:00,  1.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null Value Handling Done!\n",
      "Scaling and Encoding Done!\n",
      "Tensor Conversion and dataset prep done!!Input Size:11\n",
      "World Size: 1\n",
      "[Rank 0] Initialization complete. Using world size 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Rank 0] Epoch 1 average loss: 0.3031605256538253\n",
      "[Rank 0] Epoch 1 validation loss: 0.24297098016456214\n",
      "[Rank 0] Epoch 1 validation accuracy: 92.0237\n",
      "Rank 0 and Epoch 1 Time Taken:58.58 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 1/15 [00:58<13:40, 58.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Rank 0] Epoch 2 average loss: 0.2462173060830692\n",
      "[Rank 0] Epoch 2 validation loss: 0.23976676132090974\n",
      "[Rank 0] Epoch 2 validation accuracy: 92.1317\n",
      "Rank 0 and Epoch 2 Time Taken:83.05 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 2/15 [02:21<15:48, 72.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Rank 0] Epoch 3 average loss: 0.24186384169942293\n",
      "[Rank 0] Epoch 3 validation loss: 0.2350219040668374\n",
      "[Rank 0] Epoch 3 validation accuracy: 92.0623\n",
      "Rank 0 and Epoch 3 Time Taken:62.00 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 3/15 [03:23<13:35, 67.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Rank 0] Epoch 4 average loss: 0.23940203913037145\n",
      "[Rank 0] Epoch 4 validation loss: 0.23801030514810306\n",
      "[Rank 0] Epoch 4 validation accuracy: 92.0680\n",
      "Rank 0 and Epoch 4 Time Taken:57.85 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 4/15 [04:21<11:43, 63.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Rank 0] Epoch 5 average loss: 0.23639223494190312\n",
      "[Rank 0] Epoch 5 validation loss: 0.261679640406128\n",
      "[Rank 0] Epoch 5 validation accuracy: 91.9397\n",
      "Rank 0 and Epoch 5 Time Taken:59.01 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 5/15 [05:20<10:21, 62.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Rank 0] Epoch 6 average loss: 0.2333888474295245\n",
      "[Rank 0] Epoch 6 validation loss: 0.31872281319758833\n",
      "[Rank 0] Epoch 6 validation accuracy: 90.1901\n",
      "Rank 0 and Epoch 6 Time Taken:58.58 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 6/15 [06:19<09:08, 60.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Rank 0] Epoch 7 average loss: 0.23009176170804754\n",
      "[Rank 0] Epoch 7 validation loss: 0.3217087206883103\n",
      "[Rank 0] Epoch 7 validation accuracy: 90.3758\n",
      "Rank 0 and Epoch 7 Time Taken:58.20 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 7/15 [07:17<08:00, 60.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Rank 0] Epoch 8 average loss: 0.22806045590040072\n",
      "[Rank 0] Epoch 8 validation loss: 0.30172603170607726\n",
      "[Rank 0] Epoch 8 validation accuracy: 91.2464\n",
      "Rank 0 and Epoch 8 Time Taken:58.25 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 8/15 [08:15<06:56, 59.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Rank 0] Epoch 9 average loss: 0.22800579647325442\n",
      "[Rank 0] Epoch 9 validation loss: 0.3463103831078011\n",
      "[Rank 0] Epoch 9 validation accuracy: 90.9043\n",
      "Rank 0 and Epoch 9 Time Taken:58.45 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 9/15 [09:13<05:54, 59.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Rank 0] Epoch 10 average loss: 0.22632484066371136\n",
      "[Rank 0] Epoch 10 validation loss: 0.3506210121694418\n",
      "[Rank 0] Epoch 10 validation accuracy: 90.1828\n",
      "Rank 0 and Epoch 10 Time Taken:58.23 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 10/15 [10:12<04:54, 58.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Rank 0] Epoch 11 average loss: 0.22597560106673745\n",
      "[Rank 0] Epoch 11 validation loss: 0.37038494929389293\n",
      "[Rank 0] Epoch 11 validation accuracy: 90.4165\n",
      "Rank 0 and Epoch 11 Time Taken:58.29 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 11/15 [11:10<03:54, 58.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Rank 0] Epoch 12 average loss: 0.22481657928580262\n",
      "[Rank 0] Epoch 12 validation loss: 0.3871530116265389\n",
      "[Rank 0] Epoch 12 validation accuracy: 90.2840\n",
      "Rank 0 and Epoch 12 Time Taken:58.25 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 12/15 [12:08<02:55, 58.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Rank 0] Epoch 13 average loss: 0.22538005609670475\n",
      "[Rank 0] Epoch 13 validation loss: 0.39002264216212684\n",
      "[Rank 0] Epoch 13 validation accuracy: 90.4901\n",
      "Rank 0 and Epoch 13 Time Taken:58.31 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 13/15 [13:07<01:56, 58.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Rank 0] Epoch 14 average loss: 0.22445262109146003\n",
      "[Rank 0] Epoch 14 validation loss: 0.38923822485760506\n",
      "[Rank 0] Epoch 14 validation accuracy: 90.2094\n",
      "Rank 0 and Epoch 14 Time Taken:57.90 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 14/15 [14:04<00:58, 58.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Rank 0] Epoch 15 average loss: 0.2235642375779405\n",
      "[Rank 0] Epoch 15 validation loss: 0.42239211968142687\n",
      "[Rank 0] Epoch 15 validation accuracy: 90.4624\n",
      "Rank 0 and Epoch 15 Time Taken:58.33 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [15:03<00:00, 60.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 0: Total training time: 903.32 seconds\n",
      "Training Done!\n",
      "Total Elapsed time: 909.73 seconds\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Load dataset from CSV\n",
    "    astro_ds = dataloading(path='./dataset.csv')\n",
    "    \n",
    "    # Handle null values and separate features and labels\n",
    "    X_imputed, y = nullvalue_handling(astro_ds)\n",
    "    \n",
    "    # Select important features based on some criteria\n",
    "    astro_ds_filtered = feature_importance(X_imputed, y)\n",
    "    \n",
    "    # Re-handle null values after filtering features\n",
    "    X_imputed, y = nullvalue_handling(astro_ds_filtered)\n",
    "    \n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_imputed, y, test_size=0.2, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Prepare data by scaling features and encoding labels\n",
    "    X_train_scaled, X_test_scaled, y_train_encoded, y_test_encoded = dataprep(X_train, X_test, y_train, y_test)\n",
    "    \n",
    "    # Convert datasets to tensor datasets suitable for PyTorch\n",
    "    train_dataset, val_dataset, input_size = tensordataset(X_train_scaled, y_train_encoded, X_test_scaled, y_test_encoded)\n",
    "\n",
    "    # Start timing the distributed training setup\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Determine the number of GPUs available\n",
    "    world_size = torch.cuda.device_count()\n",
    "    print(f\"World Size: {world_size}\")\n",
    "    \n",
    "    processes = []\n",
    "    # Create a separate process for each GPU to handle training\n",
    "    for rank in range(world_size):\n",
    "        p = torch.multiprocessing.Process(target=train, args=(rank, world_size, input_size, train_dataset, val_dataset, 32, 15))\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "    \n",
    "    # Wait for all processes to finish\n",
    "    for p in processes:\n",
    "        p.join()\n",
    "\n",
    "    # Calculate and print the total elapsed time for training\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Total Elapsed time: {elapsed_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c291fc7-064a-41a8-97f1-c2110153750c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
